go: https://golang.org/doc/install?download=go1.15.darwin-amd64.pkg

Go to outer folder:
 make
to build the pkg

the output will be in bin folder: tidb-server

interesting atom based golang ide: https://atom.io/packages/ide-golang




=========================================================================
design info:

storage: mvcc as usual, version + key as primary key for kv store
 - kv stored ordered by key's binary order
 - RocksDB is used as local kv storage and interact with disk (same as other similiar spanner fans it seems)
 - kv store is done by sharding, the sharding is based on a range key instead of hashing data to nodes, Each segment is called a Region, Each Region can be described by [StartKey, EndKey)
 - PD is responsible for gathering data on which region maps to which node
 - by default all read/writes goes to raft leader!!!!???
 - good mvcc start point doc: https://pingcap.com/blog/2016-11-17-mvcc-in-tikv
 
 
 - replication:
   - raft is used for replication https://docs.pingcap.com/tidb/dev/tidb-storage
   - raft: https://raft.github.io/raft.pdf

trasntraction:
1. optimistic concurrent control: write to memory, commit by 2pc
 - if db and storage are both down, not sure what will happen
2. pemissitic concurrent control: row level locking, no gap lock, support read committed and repeatedable read
 - repeatable read: this is following the strategy of timestamp from PD.
 - as all trnastraction executions start with getting a timestamp from PD, they are ordered as well as all the records, it is not very hard to achieve what should be read
   and what shouldn't

PD: placement driver
 - this is critical as it is used to gather the timestamp. this must be availiable across all failures otherwise the system doesn't work at all.

“The first issue to be resolved is the single point of failure of PD. Our solution is to start multiple PD servers. These servers elect a Leader through the election mechanism in etcd and the leader provides services to the outside. ”
https://pingcap.com/blog/2016-11-09-Deep-Dive-into-TiKV#placement-driver

It seems there is no highly capabily load balanceing logic to use multiple PD server while achieving consistency => which is another distributed transtraction scenario


