go: https://golang.org/doc/install?download=go1.15.darwin-amd64.pkg

Go to outer folder:
 make
to build the pkg

the output will be in bin folder: tidb-server

interesting atom based golang ide: https://atom.io/packages/ide-golang




=========================================================================
design info:

storage: mvcc as usual, version + key as primary key for kv store
 - kv stored ordered by key's binary order
 - RocksDB is used as local kv storage and interact with disk (same as other similiar spanner fans it seems)
 - kv store is done by sharding, the sharding is based on a range key instead of hashing data to nodes, Each segment is called a Region, Each Region can be described by [StartKey, EndKey)
 - PD is responsible for gathering data on which region maps to which node
 - by default all read/writes goes to raft leader!!!!???
 - good mvcc start point doc: https://pingcap.com/blog/2016-11-17-mvcc-in-tikv
 
 
 - replication:
   - raft is used for replication https://docs.pingcap.com/tidb/dev/tidb-storage
   - raft: https://raft.github.io/raft.pdf

trasntraction:
1. optimistic concurrent control: write to memory, commit by 2pc
 - if db and storage are both down, not sure what will happen
2. pemissitic concurrent control: row level locking, no gap lock, support read committed and repeatedable read
 - repeatable read: this is following the strategy of timestamp from PD.
 - as all trnastraction executions start with getting a timestamp from PD, they are ordered as well as all the records, it is not very hard to achieve what should be read
   and what shouldn't

PD: placement driver
 - this is critical as it is used to gather the timestamp. this must be availiable across all failures otherwise the system doesn't work at all.

“The first issue to be resolved is the single point of failure of PD. Our solution is to start multiple PD servers. These servers elect a Leader through the election mechanism in etcd and the leader provides services to the outside. ”
https://pingcap.com/blog/2016-11-09-Deep-Dive-into-TiKV#placement-driver

It seems there is no highly capabily load balanceing logic to use multiple PD server while achieving consistency => which is another distributed transtraction scenario

=================================
Difference of TIDB, cockroachDB and yugabyteDB on timing part

TIDB: use placement driver to provide incremental timestamp for all transtractions (single point, potential bottle neck especially for cross data ceneter db cluster)

cockroachDB: uses  Hybrid-Logical Clocks, no obvious bottle neck on scaling as no single server, increased retry count as the model requires "Uncertainty Intervals", if other changes is observed in such interval, the transtraction needs rollback, update time stamp and retry
- potentially higher latency if we have large amount of conflict
- something is added in "Behavior under Clock Skew" in "CockroachDB: The Resilient Geo-Distributed SQL Database" to gard when it actualy have higher timerskew then we expect
  - something: an epoch based lease
      - 1. for each range (raft group), a time stamp range it can serve is added. All the ranges are not overlapping
      - 2. for each trans touches the range, it stores the leaseholder epoch with the changes, upon commit the epoch is verified, abort is needed if lease holder changed
  - this prevents breaking isolation for write as we guarantee writes are not coming from 2 leader at same time.  but not staled read (say 2nd trans have timestamp lower than first trans time - interval)
  = note that we can do this in each range so we don't need to worry about cross range trans, as each range is consistent, and the overall trans aborts if it has partial failure

In trans handling, a transaction record stored in the range where the first write occurs, lock is created separately in provisional value (write, persist but not committed, in separate rocksDB process)
https://www.cockroachlabs.com/docs/v20.1/architecture/transaction-layer.html
The time stamp is chosed by the gateway node

how to find who is the leader of each group?
"Meta ranges
The locations of all ranges in your cluster are stored in a two-level index at the beginning of your key-space, known as meta ranges, where the first level (meta1) addresses the second, and the second (meta2) addresses data in the cluster. Importantly, every node has information on where to locate the meta1 range (known as its range descriptor, detailed below), and the range is never split."

yugabyteDB
 YB-Master Service : metadata, placement etc, usign raft and leader election, serves request like: where is this tablet?
 YB-TServer Service : storage io
 tablet <-> range
 
 tablet raft leader choose "hybrid timestamp" for each write: https://docs.yugabyte.com/latest/architecture/transactions/transactions-overview/#hybrid-logical-clocks
 Hybrid Logical Clock is used again
 
 
Seems in storage layer a macheminism is implemented to support transtraction https://docs.yugabyte.com/latest/architecture/transactions/distributed-txns/
https://docs.yugabyte.com/latest/architecture/transactions/transactional-io-path/

It has a special tablet to store the records of trans info, and the trasn info is replicated via raft, (might have single host spot for this tablet raft leader)
 


to read:
https://doc.nuodb.com/nuodb/latest/release-notes/
